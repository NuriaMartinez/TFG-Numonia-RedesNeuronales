{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7176c0-5ed5-4850-a27a-b820414ff96d",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" width=\"40%\" src=\"pics/Universidad Burgos.png\">\n",
    "<img style=\"float:right\" width=\"16%\" src=\"pics/person1_bacteria_2.jpeg\">\n",
    "\n",
    "<br style=\"clear:both;\">\n",
    "\n",
    "# Trabajo Fin de Grado\n",
    "\n",
    "<h2 style=\"display: inline-block; padding: 4mm; padding-left: 2em; background-color: navy; line-height: 1.3em; color: white; border-radius: 10px;\">NOMBRE TFG</h2>\n",
    "\n",
    "### Nuria Martínez Queralt\n",
    "\n",
    "### Grado en Ingeniería de la Salud \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33e68c-4e8b-4305-91c2-cff54089bbf4",
   "metadata": {},
   "source": [
    "En este notebook se han llevado a cabo una serie de tareas para la realización del TFG, el cual consiste en la identificación de neumonía a partir de radiografías de tórax empleando una red neuronal. Para esto, se deben probar distintos modelos hasta llegar al modelo más optimo de red neuronal para este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296a9ed-5af7-4b30-91c5-7015aefef326",
   "metadata": {},
   "source": [
    "## Redistribución de las imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1466b-002c-4d9e-8c11-fe752738fabe",
   "metadata": {},
   "source": [
    "Debido a que la distribución inicial obtenida a partir del dataset descargado de internet: \"https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\" incluye únicamente 16 imágenes en la carpeta de validación (\"val\") y, esto supone problemas para la obtención de buenos resultados a la hora de construir nuestra red neuronal, antes de empezar a trabajar con las imágenes, se debe crear una función para obtener un nuevo dataset con nuevas carpetas \"train\", \"test\" y \"val\" y una nueva distribución de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ba6e4b-f179-42a1-bcf7-6a4b8a20f3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV guardado en: C:/Users/nuria/Downloads/TFG/data\\dataset_info.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Se crea un csv con dos columnas nombres_ficheros y clases. En la columna nombres_ficheros debe aparecer el nombre de TODAS \n",
    "las imágenes que existen dentro de cada subcarpeta y en la columna clases debe aparecer 0 o 1 en función si se trata de una imágenes \n",
    "de una de las carpetas de NORMAL o PNEUMONIA respectivamente\n",
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ruta donde se encuentra el dataset con la distribución inicial descargada de internet\n",
    "directorio_padre='C:/Users/nuria/Downloads/TFG/data'\n",
    "\n",
    "# Listas para almacenar los nombres de las imágenes y las clases (0 o 1 en función de si es normal o neumonía respectivamente)\n",
    "nombres_ficheros = []\n",
    "clases = []\n",
    "\n",
    "# Recorremos las carpetas de train, test y val\n",
    "for subcarpeta in ['train', 'test', 'val']:\n",
    "    ruta_subcarpeta = os.path.join(directorio_padre, subcarpeta)\n",
    "    for clase in ['NORMAL', 'PNEUMONIA']:\n",
    "        ruta_clase = os.path.join(ruta_subcarpeta, clase)\n",
    "        for nombre_fichero in os.listdir(ruta_clase):\n",
    "            #CREO QUE LA SIGUIENTE LINEA NO HACE FALTA\n",
    "            #if nombre_fichero.endswith(('.png', '.jpg', '.jpeg')):  # Puedes ajustar esto a tus extensiones de imagen\n",
    "            nombres_ficheros.append(nombre_fichero)\n",
    "            clases.append(0 if clase == 'NORMAL' else 1)\n",
    "\n",
    "# Se crea el DataFrame con los datos\n",
    "df_todas = pd.DataFrame({'nombre_fichero': nombres_ficheros,'clase': clases})\n",
    "\n",
    "# Se guarda el DataFrame en un archivo CSV\n",
    "ruta_csv = os.path.join(directorio_padre, 'dataset_info.csv')\n",
    "df_todas.to_csv(ruta_csv, index=False)\n",
    "\n",
    "print(f'Archivo CSV guardado en: {ruta_csv}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e931e6c9-e9b0-4313-b55f-815240ff2398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre_fichero</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IM-0115-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IM-0117-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IM-0119-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IM-0122-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IM-0125-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851</th>\n",
       "      <td>person1949_bacteria_4880.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5852</th>\n",
       "      <td>person1950_bacteria_4881.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5853</th>\n",
       "      <td>person1951_bacteria_4882.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5854</th>\n",
       "      <td>person1952_bacteria_4883.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5855</th>\n",
       "      <td>person1954_bacteria_4886.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5856 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     nombre_fichero  clase\n",
       "0                 IM-0115-0001.jpeg      0\n",
       "1                 IM-0117-0001.jpeg      0\n",
       "2                 IM-0119-0001.jpeg      0\n",
       "3                 IM-0122-0001.jpeg      0\n",
       "4                 IM-0125-0001.jpeg      0\n",
       "...                             ...    ...\n",
       "5851  person1949_bacteria_4880.jpeg      1\n",
       "5852  person1950_bacteria_4881.jpeg      1\n",
       "5853  person1951_bacteria_4882.jpeg      1\n",
       "5854  person1952_bacteria_4883.jpeg      1\n",
       "5855  person1954_bacteria_4886.jpeg      1\n",
       "\n",
       "[5856 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_todas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a13ab911-10e1-493d-bf1f-967e5f801fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DENTRO DE LAS CARPETAS PNEUMONIA, SE PUEDE TENER EN CUENTA VIRAL Y BACTERIANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdac866-53de-4c70-9bfc-2042441bf0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV de entrenamiento guardado en: C:/Users/nuria/Downloads/TFG/data/train_dataset_info.csv\n",
      "Archivo CSV de prueba guardado en: C:/Users/nuria/Downloads/TFG/data/test_dataset_info.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A partir del csv anterior y, con ayuda de la función train_test_split de skitlearn de debe dividir el csv anterior en dos \n",
    "subgrupos de train y test en proporción 80, 20 para poder usar el 80% de las imágenes para train y el 20% para test.\n",
    "También se emplea el parámetro stratify para que también exista una proporción de clases en cada uno de los grupos.\n",
    "Posteriormente se deberá dividir el conjunto de train en 2 para obtener así el subconjunto de validación.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# se emplea train_test_split para dividir el dataset en train (80%) y test (20%)\n",
    "# random_state=42 se emplea para que cada vez que se ejecute el código, se obtenga la misma división de datos. El valor 42 es un valor que se usa\n",
    "# comunmente en este caso pero se puede emplear cualquie otro valor entero\n",
    "# stratify se emplea para agrupar de manera proporcional las clases neumonia y normal en los distintos dataframes\n",
    "train_df, test_df = train_test_split(df_todas, test_size=0.2, stratify=df_todas['clase'], random_state=42)\n",
    "\n",
    "# Se guardan los nuevos conjuntos de datos en archivos CSV\n",
    "ruta_train_csv = 'C:/Users/nuria/Downloads/TFG/data/train_dataset_info.csv' #cambiar ruta en caso necesario (lo ultimo es el nombre del nuevo)\n",
    "ruta_test_csv = 'C:/Users/nuria/Downloads/TFG/data/test_dataset_info.csv' #cambiar ruta en caso necesario (lo ultimo es el nombre del nuevo)\n",
    "train_df.to_csv(ruta_train_csv, index=False)\n",
    "test_df.to_csv(ruta_test_csv, index=False)\n",
    "\n",
    "print(f'Archivo CSV de entrenamiento guardado en: {ruta_train_csv}')\n",
    "print(f'Archivo CSV de prueba guardado en: {ruta_test_csv}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4311ac3-8915-405f-bcfb-5b6f3000e655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre_fichero</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>person80_bacteria_391.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>person1051_virus_1750.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>person734_bacteria_2637.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3695</th>\n",
       "      <td>person410_bacteria_1825.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>person1320_bacteria_3347.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3896</th>\n",
       "      <td>person462_bacteria_1968.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>NORMAL2-IM-0944-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3554</th>\n",
       "      <td>person371_bacteria_1701.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>person1518_virus_2645.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>person301_virus_622.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4684 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     nombre_fichero  clase\n",
       "5786     person80_bacteria_391.jpeg      1\n",
       "1427     person1051_virus_1750.jpeg      1\n",
       "4746   person734_bacteria_2637.jpeg      1\n",
       "3695   person410_bacteria_1825.jpeg      1\n",
       "1987  person1320_bacteria_3347.jpeg      1\n",
       "...                             ...    ...\n",
       "3896   person462_bacteria_1968.jpeg      1\n",
       "989       NORMAL2-IM-0944-0001.jpeg      0\n",
       "3554   person371_bacteria_1701.jpeg      1\n",
       "2472     person1518_virus_2645.jpeg      1\n",
       "3252       person301_virus_622.jpeg      1\n",
       "\n",
       "[4684 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171451c6-6c4e-4d6b-9477-3cb6ae30c257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre_fichero</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>person294_bacteria_1388.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>person321_bacteria_1483.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>person1372_bacteria_3502.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>person1484_bacteria_3878.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>IM-0346-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4445</th>\n",
       "      <td>person5_bacteria_15.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326</th>\n",
       "      <td>NORMAL2-IM-0135-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4764</th>\n",
       "      <td>person740_virus_1362.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>person646_bacteria_2538.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>person549_virus_1089.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1172 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     nombre_fichero  clase\n",
       "3209   person294_bacteria_1388.jpeg      1\n",
       "3334   person321_bacteria_1483.jpeg      1\n",
       "2119  person1372_bacteria_3502.jpeg      1\n",
       "2384  person1484_bacteria_3878.jpeg      1\n",
       "160               IM-0346-0001.jpeg      0\n",
       "...                             ...    ...\n",
       "4445       person5_bacteria_15.jpeg      1\n",
       "5326      NORMAL2-IM-0135-0001.jpeg      0\n",
       "4764      person740_virus_1362.jpeg      1\n",
       "4573   person646_bacteria_2538.jpeg      1\n",
       "4261      person549_virus_1089.jpeg      1\n",
       "\n",
       "[1172 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87fdd81-31fa-4600-8370-be9579e95ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV de entrenamiento final guardado en: C:/Users/nuria/Downloads/TFG/data/train_final_dataset_info.csv\n",
      "Archivo CSV de validación guardado en: C:/Users/nuria/Downloads/TFG/data/val_dataset_info.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A continuación, se coge el conjunto de datos obtenido previamente de train, es decir, el csv \"train_df\" y se repite el mismo\n",
    "proceso pero, esta vez dividiendo este conjunto de datos para train y val en un 80% y 20% respectivamente.\n",
    "De tal forma que, finalemnte se obtenga el conjunto de test que represneta el 20% del total (obtenido previamente), el conjunto de train\n",
    "que representa el 80% del 80% del total ya que, inicialmente nos hemos quedado con el 80% pero luego, de este 80%, el 20% va destinado al conjunto\n",
    "de validación.\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Se emplea train_test_split para dividir el conjunto de datos de entrenamiento en train (80%) y val (20%)\n",
    "train_def_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['clase'], random_state=42)\n",
    "\n",
    "# Se guardan los nuevos conjuntos de datos en archivos CSV\n",
    "ruta_train_final_csv = 'C:/Users/nuria/Downloads/TFG/data/train_final_dataset_info.csv' #cambiar ruta en caso necesario (lo ultimo es el nombre del nuevo)\n",
    "ruta_val_csv = 'C:/Users/nuria/Downloads/TFG/data/val_dataset_info.csv' #cambiar ruta en caso necesario (lo ultimo es el nombre del nuevo)\n",
    "train_def_df.to_csv(ruta_train_final_csv, index=False)\n",
    "val_df.to_csv(ruta_val_csv, index=False)\n",
    "\n",
    "print(f'Archivo CSV de entrenamiento final guardado en: {ruta_train_final_csv}')\n",
    "print(f'Archivo CSV de validación guardado en: {ruta_val_csv}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2516a642-8ce8-4240-8762-918355fb33a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre_fichero</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>NORMAL2-IM-0448-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>person266_virus_549.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>IM-0666-0001-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>person1411_bacteria_3610.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>NORMAL2-IM-0468-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4830</th>\n",
       "      <td>person772_virus_1401.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>person354_bacteria_1635.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>person1180_virus_2012.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>person1216_bacteria_3168.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>person543_bacteria_2282.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3747 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     nombre_fichero  clase\n",
       "649       NORMAL2-IM-0448-0001.jpeg      0\n",
       "3096       person266_virus_549.jpeg      1\n",
       "513          IM-0666-0001-0001.jpeg      0\n",
       "2205  person1411_bacteria_3610.jpeg      1\n",
       "665       NORMAL2-IM-0468-0001.jpeg      0\n",
       "...                             ...    ...\n",
       "4830      person772_virus_1401.jpeg      1\n",
       "3485   person354_bacteria_1635.jpeg      1\n",
       "1691     person1180_virus_2012.jpeg      1\n",
       "1748  person1216_bacteria_3168.jpeg      1\n",
       "4228   person543_bacteria_2282.jpeg      1\n",
       "\n",
       "[3747 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_def_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74986d04-2924-4054-8d07-8cf4937258a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre_fichero</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>person1377_virus_2369.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>person326_bacteria_1506.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>person30_bacteria_151.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>person113_virus_215.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>person1070_virus_1773.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>NORMAL2-IM-0946-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5234</th>\n",
       "      <td>IM-0025-0001.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>person1170_virus_1970.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>person127_bacteria_602.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5024</th>\n",
       "      <td>person895_virus_1547.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    nombre_fichero  clase\n",
       "2130    person1377_virus_2369.jpeg      1\n",
       "3359  person326_bacteria_1506.jpeg      1\n",
       "3281    person30_bacteria_151.jpeg      1\n",
       "1588      person113_virus_215.jpeg      1\n",
       "1459    person1070_virus_1773.jpeg      1\n",
       "...                            ...    ...\n",
       "991      NORMAL2-IM-0946-0001.jpeg      0\n",
       "5234             IM-0025-0001.jpeg      0\n",
       "1662    person1170_virus_1970.jpeg      1\n",
       "5531   person127_bacteria_602.jpeg      1\n",
       "5024     person895_virus_1547.jpeg      1\n",
       "\n",
       "[937 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85a0d44-b744-4609-811e-214b7007ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Función para encontrar una imagen concreta (a prtir de su nombre) dentro de cualquiera de las subcarpetas\n",
    "Esta función es de gran interés para el apartado que se hace a continuación ya que, para encontrar la ruta_origen de la imagen\n",
    "no se sabe en que carpeta está concretamente y por tanto es necesario acceder a su ruta a partir de esta función\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "def buscar_imagen(directorio_padre, nombre_imagen):\n",
    "    # Subcarpetas principales en las que buscar\n",
    "    subcarpetas_principales = ['train', 'test', 'val']\n",
    "    # Subcarpetas adicionales en las que buscar dentro de cada subcarpeta principal\n",
    "    subcarpetas_adicionales = ['NORMAL', 'PNEUMONIA']\n",
    "\n",
    "    # Se itera sobre las subcarpetas principales\n",
    "    for subcarpeta_principal in subcarpetas_principales:\n",
    "        # Se itera sobre las subcarpetas adicionales dentro de cada subcarpeta principal\n",
    "        for subcarpeta_adicional in subcarpetas_adicionales:\n",
    "            # Se obtiene la ruta completa de la imagen\n",
    "            ruta_imagen = os.path.join(directorio_padre, subcarpeta_principal, subcarpeta_adicional, nombre_imagen)\n",
    "            # verificar si la imagen existe en la subcarpeta actual\n",
    "            if os.path.exists(ruta_imagen):\n",
    "                return ruta_imagen  # devolver la ruta de la imagen si se encuentra\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b249f54c-e604-4239-8be9-c44bde2eb53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Finalmente, se crea una nueva carpeta denominada data_nuevo, dentro de esta carpeta se crean 3 subcarpetas (\"train\", \"test\" y \"val\")\n",
    "que corresponderian con los dataframes obtenidos hasta hora: train_def_df, val_df y test_df y, dentro de estas 3 subcarpetas, se crean\n",
    "2 carpetas \"NORMAL\" y \"PNEUMONIA\" que corresponden con con las clases determinadas en cada dataframe, 0 en caso de \"NORMAL\" y 1 para \"PNEUMONIA\".\n",
    "Dentro de estas dos carpetas para (\"train\", \"test\" y \"val\") se encontraran las imagenes correspondientes para cada caso según los dataframes obtenidos.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Ruta principal donde se crearán las nuevas carpetas\n",
    "ruta_principal_nueva = 'C:/Users/nuria/Downloads/TFG/data_nuevo'\n",
    "\n",
    "\n",
    "# Se crean las carpetas 'train', 'test' y 'val' dentro de la carpeta principal\n",
    "for subcarpeta in ['train', 'test', 'val']:\n",
    "    ruta_subcarpeta = os.path.join(ruta_principal_nueva, subcarpeta)\n",
    "    os.makedirs(ruta_subcarpeta, exist_ok=True) #verifica si la carpeta ruta_subcarpeta ya existe. Si existe, no se hace nada y el programa continúa su ejecución sin lanzar un error. Si no existe, la función os.makedirs() la crea junto con cualquier carpeta intermedia necesaria en la ruta especificada\n",
    "    \n",
    "    # Se crean las subcarpetas 'normal' y 'neumonia' dentro de cada subcarpeta ('train', 'test' y 'val')\n",
    "    for clase in ['NORMAL', 'PNEUMONIA']:\n",
    "        ruta_clase = os.path.join(ruta_subcarpeta, clase)\n",
    "        os.makedirs(ruta_clase, exist_ok=True)\n",
    "\n",
    "            \n",
    "# Se copian los archivos CSV a las subcarpetas correspondientes\n",
    "for df, nombre_carpeta in [(train_def_df, 'train'), (val_df, 'val'), (test_df, 'test')]:\n",
    "    for index, row in df.iterrows(): #se itera sobre cada dataframe fila a fila\n",
    "        clase = 'NORMAL' if row['clase'] == 0 else 'PNEUMONIA'\n",
    "        nombre_archivo = row['nombre_fichero']\n",
    "\n",
    "        # ruta de origen donde se busca la imagen concreta a partir de la función realizada previamente\n",
    "        # esta ruta se refiere a donde esta que se desea guardar en la carpeta destino originalmente para poder copiarla\n",
    "        ruta_origen=buscar_imagen('C:/Users/nuria/Downloads/TFG/data', nombre_archivo)\n",
    "        \n",
    "        # ruta donde se desa guardar (y redestribuir de la forma correcta) las imágenes\n",
    "        ruta_destino = os.path.join(ruta_principal_nueva, nombre_carpeta, clase, nombre_archivo)\n",
    "        \n",
    "        shutil.copyfile(ruta_origen, ruta_destino) # copia las imágenes de la ruta incial a la ruta final\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83824e1e-bc9c-4a2d-a338-278e53c86c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#METER ESTO EN UNA O VARIAS FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c768dd-149e-48d4-85ef-0dc6bdb9a92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d4338-95ce-4ce4-828a-6fb26d0b3182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9d5cf-a0c2-4962-b808-b1e1cd129ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb0b4967-0b21-4bea-a7dd-96dfc0bc0206",
   "metadata": {},
   "source": [
    "A partir de aqui, se va a trabajar con la nueva carpeta de imágenes y su nueva distribución para evitar errores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014f5be-2c05-498e-88e7-da98e1210f1c",
   "metadata": {},
   "source": [
    "## Preparación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd3214-3749-4449-a6ce-2150370b3359",
   "metadata": {},
   "source": [
    "Se prepara el modelo para poder trabajar con las imágenes de train, test y val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f44c95-c38d-4885-a5e7-fe86df3e6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/paola311/clasificaci-n-de-im-genes-cnn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "def preparar_modelo(ruta, batch_size):\n",
    "\n",
    "    '''\n",
    "    Función que configura los generadores de datos para entrenar, validar y probar un modelo de aprendizaje automático con imágenes.\n",
    "    -----------------------------------------------------------\n",
    "    Parámetros:\n",
    "    - ruta: str. Ruta base donde se encuentran las imágenes organizadas en subcarpetas (train, val, test)\n",
    "    - batchsize: int. Tamaño del lote que se utiliza en una única iteración del algoritmo de aprendizaje\n",
    "    ----------------------------------------------------\n",
    "    Return:\n",
    "    - nada\n",
    "    '''\n",
    "    \n",
    "    dir_general = ruta #ubicacion donde se encuentran las imágenes organizadas en subcarpetas (train, val, test). Añadir esta carpeta a one drive en TFG\n",
    "\n",
    "    dir_train = os.path.join(dir_general, 'train')\n",
    "    dir_validation = os.path.join(dir_general, 'val')\n",
    "    dir_test = os.path.join(dir_general, 'test')\n",
    "    \n",
    "    # Preprocesamiento de imágenes\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    validation_datagen=ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    #Iterador que recorre el directorio de imágenes\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        dir_train,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size, #lo más grande posible que no cause problemas de memoria \n",
    "        class_mode='binary')\n",
    "    \n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        dir_validation,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size, #lo más grande posible que no cause problemas de memoria \n",
    "        class_mode='binary')\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        dir_test,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size, #lo más grande posible que no cause problemas de memoria \n",
    "        class_mode='binary')\n",
    "    \n",
    "    return train_generator, validation_generator, test_generator\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c2a919-5a84-4a53-94f1-ad964d37c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3747 images belonging to 2 classes.\n",
      "Found 937 images belonging to 2 classes.\n",
      "Found 1172 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "ruta='C:/Users/nuria/Downloads/TFG/data_nuevo'\n",
    "batch_size=20\n",
    "\n",
    "train_generator, validation_generator, test_generator = preparar_modelo(ruta, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d31e2-5b84-48e0-add1-06484c34f7d2",
   "metadata": {},
   "source": [
    "## Matriz de confusión para ver como funciona el modelo más simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2edc6db-723d-4830-8618-9e9ac1fe251b",
   "metadata": {},
   "source": [
    "Se obtiene la matriz de confusión para un primer modelo muy simple para, así poder comprobar como estos resultados mejoran al introducir capas ocultas, modificar parámetros..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c0d8bc-a780-43f7-afa1-1b4ebfbe668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 82944)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 82944)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 82945     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,337\n",
      "Trainable params: 102,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#se trabaja con el modelo más simple (posteriormente denominado Simple1)\n",
    "input_shape=(150,150,3)\n",
    "\n",
    "model = keras.Sequential( \n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"), \n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5), \n",
    "        layers.Dense(1, activation=\"sigmoid\"), #una unica neurina, sigmoide\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cad93d8-8ea8-4134-b9ee-436c775e8934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "188/188 [==============================] - 126s 655ms/step - loss: 0.2915 - accuracy: 0.8703 - recall: 0.9451 - auc: 0.9285 - val_loss: 0.1568 - val_accuracy: 0.9456 - val_recall: 0.9635 - val_auc: 0.9786\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 124s 659ms/step - loss: 0.1357 - accuracy: 0.9512 - recall: 0.9718 - auc: 0.9847 - val_loss: 0.2976 - val_accuracy: 0.8837 - val_recall: 0.8523 - val_auc: 0.9790\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 125s 666ms/step - loss: 0.1244 - accuracy: 0.9533 - recall: 0.9733 - auc: 0.9865 - val_loss: 0.2051 - val_accuracy: 0.9360 - val_recall: 0.9298 - val_auc: 0.9791\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 118s 629ms/step - loss: 0.1184 - accuracy: 0.9586 - recall: 0.9773 - auc: 0.9875 - val_loss: 0.1557 - val_accuracy: 0.9456 - val_recall: 0.9532 - val_auc: 0.9807\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 119s 631ms/step - loss: 0.0887 - accuracy: 0.9698 - recall: 0.9824 - auc: 0.9929 - val_loss: 0.1576 - val_accuracy: 0.9434 - val_recall: 0.9649 - val_auc: 0.9785\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 122s 647ms/step - loss: 0.0808 - accuracy: 0.9704 - recall: 0.9813 - auc: 0.9941 - val_loss: 0.1618 - val_accuracy: 0.9434 - val_recall: 0.9635 - val_auc: 0.9783\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 124s 661ms/step - loss: 0.0619 - accuracy: 0.9776 - recall: 0.9872 - auc: 0.9961 - val_loss: 0.1801 - val_accuracy: 0.9434 - val_recall: 0.9591 - val_auc: 0.9768\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 120s 639ms/step - loss: 0.0473 - accuracy: 0.9827 - recall: 0.9883 - auc: 0.9974 - val_loss: 0.2234 - val_accuracy: 0.9360 - val_recall: 0.9678 - val_auc: 0.9678\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 118s 624ms/step - loss: 0.0460 - accuracy: 0.9840 - recall: 0.9905 - auc: 0.9974 - val_loss: 0.1894 - val_accuracy: 0.9402 - val_recall: 0.9708 - val_auc: 0.9768\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 118s 629ms/step - loss: 0.0367 - accuracy: 0.9877 - recall: 0.9920 - auc: 0.9989 - val_loss: 0.2384 - val_accuracy: 0.9338 - val_recall: 0.9678 - val_auc: 0.9666\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 119s 631ms/step - loss: 0.0431 - accuracy: 0.9845 - recall: 0.9894 - auc: 0.9984 - val_loss: 0.2272 - val_accuracy: 0.9445 - val_recall: 0.9605 - val_auc: 0.9698\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 117s 625ms/step - loss: 0.0314 - accuracy: 0.9896 - recall: 0.9934 - auc: 0.9988 - val_loss: 0.2234 - val_accuracy: 0.9456 - val_recall: 0.9547 - val_auc: 0.9707\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 121s 644ms/step - loss: 0.0285 - accuracy: 0.9904 - recall: 0.9931 - auc: 0.9989 - val_loss: 0.3200 - val_accuracy: 0.9338 - val_recall: 0.9284 - val_auc: 0.9712\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - 118s 625ms/step - loss: 0.0158 - accuracy: 0.9952 - recall: 0.9963 - auc: 0.9994 - val_loss: 0.2829 - val_accuracy: 0.9413 - val_recall: 0.9751 - val_auc: 0.9595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2457aeaa3b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 32 \n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",\"Recall\",\"AUC\"]) #cambias loss\n",
    "\n",
    "# con callbacks se detiene el entrenamiento si la pérdida en el conjunto de validación no mejora después de 5 épocas (patience)\n",
    "model.fit(train_generator, epochs=epochs, validation_data=validation_generator, callbacks=EarlyStopping(monitor='val_auc', patience=10,restore_best_weights=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d499cd3b-fe4e-4a3a-87d7-791e7f119711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 28s 464ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test=test_generator.labels\n",
    "y_pred=model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d920a58c-c342-4cdc-905e-6fb20cc1cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=y_pred>0.5 #para convertirlo en un problema binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81208df-f1dd-43c4-8faf-2e511a32a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matriz de confusión con sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred) #.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad04e18-b7e3-4eb4-84f9-1c49fc4dd3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 87, 230],\n",
       "       [246, 609]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9765b599-fb6c-466a-85f0-69781acfa59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PERCEPTRON SKLEARN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import numpy as np \n",
    "\n",
    "labels=np.unique(y_test)\n",
    "\n",
    "matriz_conf = metrics.confusion_matrix(y_test, y_pred,labels=labels)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = matriz_conf, display_labels = [\"neumonía\" , \"no neumonía\"])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "cm_display.plot(ax=ax)\n",
    "plt.title(\"Matriz de confusión neumonía-no neumonía\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b896f-e060-488a-a228-64ba1e0fd5b3",
   "metadata": {},
   "source": [
    "Como se puede observar, los resultados en este primer modelo tan simple, sin ninguna capa oculta, son bastante malos ya que, poniendo esto en un caso clínico real, significaría que 230 pacientes con neumonía hubieran sido diagnosticados como no neumonía y 246 pacientes sin neumonía hubieran sido diagnosticados con neumonía, lo que supondría serios problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5654c9-4f71-4980-9ade-721a48531aec",
   "metadata": {},
   "source": [
    "## Creación de métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5c91f-3b3e-403c-820b-56c4cd7ecfa7",
   "metadata": {},
   "source": [
    "Se crea una función para calcular las distintas métricas que servirán para la posterior evaluación de cada modelo que se realice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b62f0ad-3bf5-43b6-b2a6-ae64737f2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "'''import numpy as np\n",
    "import tensorflow as tf'''\n",
    "\n",
    "def metricas(y_test, y_pred):\n",
    "    '''\n",
    "    Funcicón que calcula distintas métricas para la evaluación del modelo.\n",
    "    -----------------------------------------------------\n",
    "    Parámetros: \n",
    "    - y_test: array de etiquetas verdaderas del conjunto de prueba\n",
    "    - y_pred: array de etiquetas predichas por el modelo\n",
    "    ----------------------------------------\n",
    "    Return: \n",
    "    - accuracy: float que indica la proporción de predicciones correctas\n",
    "    - precision: float que indica la proporción de predicciones positivas correctas\n",
    "    - recall: float que indica la proporción de positivos detectados\n",
    "    - f1: float que indica la media armónica de precisión y exhaustividad para evaluar de una forma más equilibrada el rendimiento del modelo\n",
    "    - specificity: float que indica la proporción de negativos detectados\n",
    "    - fpr: float que indica la tasa de falsos positivos, es decir, la proporción de negativos incorrectamente clasificadas como positivos, \n",
    "    respecto al total de casos negativos reales.\n",
    "    - fnr: float que indica la tasa de falsos negativos, es decir, la proporción de positivos incorrectamente clasificadas como negativos, \n",
    "    respecto al total de casos positivos reales.\n",
    "    - auc: float que se emplea para evaluar la capacidad de distinción entre clases positivas y negativas de un modelo de clasificación \n",
    "    binaria. Un 1 significa que es capaz de distinguir perfectamente entre clases, un 0.5 significa una clasificación aleatoria y un 0 indica \n",
    "    que ninguna clase ha sido correctamente clasificada.\n",
    "    '''\n",
    "    y_pred=y_pred>0.5 #para convertirlo en un problema binario\n",
    "    \n",
    "    #se obtienen los verdaderos negativos, falsos positivos, falsos negativos y verdaderos positivos a partir de la matriz de confusión \n",
    "    #con .ravel() se convierte la matriz en un array unidimensional\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel() \n",
    "\n",
    "    #se calculan cada una de las métricas empleando su correspondiente fórmula\n",
    "    accuracy = (tp + tn)/(tn + fp + fn + tp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * ((precision*recall)/(precision+recall))\n",
    "    specificity = tn / (tn + fp)\n",
    "    fpr = fp / (fp + tn) #tasa de falsos positivos\n",
    "    fnr = fn / (fn + tp) #tasa de falsos negativos\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    \n",
    "    return [accuracy, precision, recall, f1, specificity, fpr, fnr, auc] #se devuleve como una lista para poder trabajar correctmante con las métricas\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e775dc-426e-447b-bc1b-35d52330027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 27s 456ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test=test_generator.labels\n",
    "y_pred=model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e59a0d8-8fe2-4590-b50d-d3e1dd996e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5870307167235495,\n",
       " 0.7210965435041716,\n",
       " 0.7076023391812866,\n",
       " 0.7142857142857142,\n",
       " 0.2618296529968454,\n",
       " 0.7381703470031545,\n",
       " 0.29239766081871343,\n",
       " 0.484715996089066]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricas(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e81a3-d57f-49a8-8307-1f4812bed084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc1b32f5-76dc-483e-bb31-8c6e0d468c1f",
   "metadata": {},
   "source": [
    "## Comparación de distintas arquitecturas de modelo y distintos batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fdfaa-3ae7-4085-8522-1d295e2ca373",
   "metadata": {},
   "source": [
    "Para realizar una comparación entre distintas arquitecturas y distintos batch_size, en primer lugar, se generan diferentes modelos de arquitectura de red neuronal variando las capas, el número de capas, etc y, después se entrenan y evalúan los modelos generados con distintos batch sizes. Para esto, se emplean las métricas previamente definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "236828d2-67b4-423a-98cf-e733615ff575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def establecer_arquitectura(tipo):\n",
    "\n",
    "    '''\n",
    "    Función que establece distintos tipos de modelos de red neuronal convolucional (CNN) según el tipo que se introduzca como parámetro.\n",
    "    --------------------------------------------------------------\n",
    "    Parámetros\n",
    "    - tipo: str que indica el tipo de modelo al que se quiere acceder \n",
    "    -------------------------------------------------------------\n",
    "    Return\n",
    "    -model: modelo sequencial en keras según el tipo que se haya introducido como parámetro de entrada y que contiene toda la información necesaria \n",
    "    sobre la arquitectura del modelo\n",
    "    '''\n",
    "    \n",
    "    input_shape=(150,150,3) # se define el tamaño de entrada de las imágenes\n",
    "\n",
    "    '''\n",
    "    El modelo Simple1, se corresponde con un modelo que posee varias capas convolucionales (con las que se obtienen características importantes\n",
    "    de las imágenes) seguidas de capas de MaxPooling2D para reducir la dimensionalidad. Después del Flatten se encuentra una capa densa.\n",
    "    La función de activación sigmoide en la capa de salida produce una probabilidad entre 0 y 1 para la clasificación binaria.\n",
    "    Este modelo es muy simple y los resultados no van a ser buenos.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    \n",
    "    if tipo == \"Simple1\":\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=input_shape),\n",
    "                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Flatten(), #convierte imágenes en vectores\n",
    "                layers.Dropout(0.2), #cambiar a menos de 0,5 \n",
    "                layers.Dense(1, activation=\"sigmoid\"), #produce una probabilidad entre 0 y 1 para la clasificación binaria\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "    El modelo Simple2, se corresponde con un modelo que posee varias capas convolucionales (con las que se obtienen características importantes\n",
    "    de las imágenes) seguidas de capas de MaxPooling2D para reducir la dimensionalidad. Después del Flatten se encuentra una capa oculta de \n",
    "    100 unidades y una capa densa.\n",
    "    La función de activación sigmoide en la capa de salida produce una probabilidad entre 0 y 1 para la clasificación binaria.\n",
    "    '''\n",
    "\n",
    "    elif tipo == \"Simple2\":\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=input_shape),\n",
    "                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Flatten(), #convierte imágenes en vectores\n",
    "                layers.Dense(100, activation=\"relu\"), #100 neuronas en la primera capa\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(1, activation=\"sigmoid\"), #produce una probabilidad entre 0 y 1 para la clasificación binaria\n",
    "            ]\n",
    "        )\n",
    "        '''\n",
    "    El modelo Simple3, se corresponde con un modelo que posee varias capas convolucionales (con las que se obtienen características importantes\n",
    "    de las imágenes) seguidas de capas de MaxPooling2D para reducir la dimensionalidad. Después del Flatten se encuentra una capa se encuentra \n",
    "    una capa oculta de 100 neuronas, una segunda capa oculta de 16 neuronas y una capa densa.\n",
    "    La función de activación sigmoide en la capa de salida produce una probabilidad entre 0 y 1 para la clasificación binaria.\n",
    "    '''\n",
    "\n",
    "    elif tipo == \"Simple3\":\n",
    "        model = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=input_shape),\n",
    "                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                layers.Flatten(), #convierte imágenes en vectores\n",
    "                layers.Dense(100, activation=\"relu\"), #100 neuronas en la primera capa\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(16, activation=\"relu\"), #16 neuronas en la segunda capa\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(1, activation=\"sigmoid\"), #produce una probabilidad entre 0 y 1 para la clasificación binaria\n",
    "            ]\n",
    "        )\n",
    "    else: #si no se cumple ninguna de las opciones anteriores, aparece un error\n",
    "        raise ValueError(\"Tipo de arquitectura no reconocida\")\n",
    "    \n",
    "    return model #model.summary??\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b7f36-6297-43d4-a8a8-93964cc84ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBA/EJECUTAR 2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def arq_batch(ruta,epochs,batch_sizes,modelos):\n",
    "    '''\n",
    "    Función que devuelve una tabla comparativa para distintas arquitecturas de modelo y distintos batch size introducidos como parámetros. \n",
    "    ----------------------------------------------------\n",
    "    Parámetros:\n",
    "    - ruta: str. Ruta base donde se encuentran las imágenes organizadas en subcarpetas (train, val, test)\n",
    "    - epochs: int. Número de épocas a entrenar \n",
    "    - batch_sizes: lista con los distintos valores de batch size para probar en cada entrenamiento\n",
    "    - modelos: lista de nombres de cada uno de los modelos que se van a comparar obtenidos partir de la función realizada previamente \n",
    "    \"establecer_arquitectura(modelo)\"\n",
    "    --------------------------------------------------\n",
    "    Return:\n",
    "    - compara_arqu_batch_def: dataframe que contiene como índice las columnas referidas al modelo de arquitectura y al valor de batch size. El dataframe \n",
    "    obtenido se observa como una tabla comparativa de diversas métricas para cada arquitectura y cada batch size.\n",
    "    '''\n",
    "    \n",
    "    #se inicializa un dataframe vacío donde, posteriormente se van a añadir todos los componentes necesarios para comparar los distintos \n",
    "    #modelos de arquitectura para distintos batch size (comparando las métricas)\n",
    "    compara_arqu_batch=pd.DataFrame()\n",
    "    \n",
    "\n",
    "    #bucle en el que se recorren cada uno de los modelos y los tamaños de batch_size \n",
    "    for modelo in modelos:\n",
    "        print(f\"Comparando modelo {modelo}...\")\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Entrenando modelo {modelo} y batch_size {batch_size}\")\n",
    "    \n",
    "            #se emplea la función preparar_modelo para configurar los generadores de datos para entrenar, validar y probar \n",
    "            #un modelo de aprendizaje automático con imágenes\n",
    "            preparar_modelo(ruta, batch_size)\n",
    "            \n",
    "            #se emplea la función establecer_arquitectura para determinar el modelo con el que se trabaja cada vez\n",
    "            model = establecer_arquitectura(modelo)\n",
    "            \n",
    "            #se compila el modelo y se calculan las métricas con las que se quiere trabajar\n",
    "            #en este caso, en la función de pérdida \"loss\", se emplea la entropía cruzada binaria \"binary_crossentropy\" ya que se trata de \n",
    "            #un problema de clasificación binaria\n",
    "            model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",\"Recall\"]) #cambias loss\n",
    "    \n",
    "            #ENTRENA\n",
    "            # con callbacks se detiene el entrenamiento si la pérdida en el conjunto de validación no mejora después de 10 épocas (patience)\n",
    "            model.fit(train_generator, epochs=epochs, validation_data=validation_generator, callbacks=EarlyStopping(monitor='val_auc', patience=10,restore_best_weights=True))\n",
    "    \n",
    "            #se calculan las métricas\n",
    "            y_test=test_generator.labels\n",
    "            y_pred=model.predict(test_generator)\n",
    "            calculo_metricas=metricas(y_test, y_pred) #se llama a la función creada previamente para calcular las métricas de cada modelo\n",
    "            #se calcula loss a partir de la evaluación del modelo\n",
    "            loss=model.evaluate(test_generator, verbose=0)[0]\n",
    "            \n",
    "            #esto es en caso de querer meter todos estos parametros dentro de metricas (cambiando tambien la linea de arriba, en lugar de metricas loss, accuracy...)\n",
    "            #metricas = f\"Loss: {loss}, Accuracy: {accuracy}, Recall: {recall}, AUC: {AUC}, Precision: {precision}\"\n",
    "    \n",
    "            #cambiar .append por .concat\n",
    "            #se añaden todos los componentes necesarios para comparar los distintos modelos de arquitectura para distintos batch size \n",
    "            #(comparando las métricas)\n",
    "            compara_arqu_batch=compara_arqu_batch.append({\"Red\": modelo, \"BatchSize\": batch_size, \"Loss\": loss, \"Accuracy\": calculo_metricas[0], \"Precision\": calculo_metricas[1], \"Recall\": calculo_metricas[2], \"F1\":calculo_metricas[3], \"Specificity\":calculo_metricas[4], \"fpr\":calculo_metricas[5], \"fnr\":calculo_metricas[6], \"AUC\": calculo_metricas[7]}, ignore_index=True)\n",
    "    \n",
    "    #se fijan las columnas Red y BatchSize como índices. \n",
    "    compara_arqu_batch.set_index([\"Red\",\"BatchSize\"], inplace=True) #inplace=True se pone para modificar el dataframe original ya que sino, no se modifica\n",
    "    compara_arqu_batch_def = compara_arqu_batch.round(2) #se redondean los decimales a 2\n",
    "    return compara_arqu_batch_def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bac317-110d-4f8c-b027-b9caee5b376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate 0,001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8be533-a397-4df4-8e10-1f131bb07c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta='C:/Users/nuria/Downloads/TFG/data_nuevo'\n",
    "epochs=20\n",
    "batch_sizes=[8, 16, 20, 32, 64]  # distintos tamaños de batch size para probar\n",
    "modelos=[\"Simple1\", \"Simple2\", \"Simple3\"]  # Lista de nombres de modelos\n",
    "arq_batch(ruta,epochs,batch_sizes,modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d50072-e379-42a4-8b45-433ce620d9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccdfc7f-173b-46f0-b974-88f4e4f099b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#PRUEBA/EJECUTAR\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "batch_sizes=[8, 16, 20, 32, 64]  # distintos tamaños de batch size para probar\n",
    "modelos=[\"Simple1\", \"Simple2\", \"Simple3\"]  # Lista de nombres de modelos\n",
    "\n",
    "#se inicializa un dataframe vacío donde, posteriormente se van a añadir todos los componentes necesarios para comparar los distintos \n",
    "#modelos de arquitectura para distintos batch size (comparando las métricas)\n",
    "compara_arqu_batch=pd.DataFrame()\n",
    "\n",
    "epochs=20\n",
    "\n",
    "#bucle en el que se recorren cada uno de los modelos y los tamaños de batch_size \n",
    "for modelo in modelos:\n",
    "    print(f\"Comparando modelo {modelo}...\")\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Entrenando modelo {modelo} y batch_size {batch_size}\")\n",
    "\n",
    "        #SE PREPARA EL MODELO\n",
    "        dir_general = 'C:/Users/nuria/Downloads/TFG/data' #ubicacion donde yo tengo metida la carpeta data (cambiar en caso necesario) y añadir esta carpeta a one drive en TFG\n",
    "\n",
    "        dir_train = os.path.join(dir_general, 'train')\n",
    "        dir_validation = os.path.join(dir_general, 'val')\n",
    "        dir_test = os.path.join(dir_general, 'test')\n",
    "\n",
    "        # Preprocesamiento de imágenes\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        validation_datagen=ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "        #Iterador que recorre el directorio de imágenes\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            dir_train,\n",
    "            target_size=(150, 150), #todas las imágenes se redimensionen a 150x150 píxeles, de forma que, si existen tamaños diferentes entre ellas, se uniforman\n",
    "            batch_size=batch_size, # se itera para distintos valores de batch size\n",
    "            class_mode='binary')\n",
    "        \n",
    "        validation_generator = validation_datagen.flow_from_directory(\n",
    "            dir_validation,\n",
    "            target_size=(150, 150), #todas las imágenes se redimensionen a 150x150 píxeles, de forma que, si existen tamaños diferentes entre ellas, se uniforman\n",
    "            batch_size=batch_size, #lo más grande posible que no cause problemas de memoria \n",
    "            class_mode='binary')\n",
    "        \n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            dir_test,\n",
    "            target_size=(150, 150), #todas las imágenes se redimensionen a 150x150 píxeles, de forma que, si existen tamaños diferentes entre ellas, se uniforman\n",
    "            batch_size=batch_size, #lo más grande posible que no cause problemas de memoria \n",
    "            class_mode='binary')\n",
    "        \n",
    "        #se emplea la función establecer_arquitectura para determinar el modelo con el que se trabaja cada vez\n",
    "        model = establecer_arquitectura(modelo)\n",
    "        \n",
    "        #se compila el modelo y se calculan las métricas con las que se quiere trabajar\n",
    "        #en este caso, en la función de pérdida \"loss\", se emplea la entropía cruzada binaria \"binary_crossentropy\" ya que se trata de \n",
    "        #un problema de clasificación binaria\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",\"Recall\"]) #cambias loss\n",
    "\n",
    "        #ENTRENA\n",
    "        # con callbacks se detiene el entrenamiento si la pérdida en el conjunto de validación no mejora después de 3 épocas (patience)\n",
    "        model.fit(train_generator, epochs=epochs, validation_data=validation_generator, callbacks=EarlyStopping(monitor='val_loss', patience=3))\n",
    "\n",
    "        #se calculan las métricas\n",
    "        y_test=test_generator.labels\n",
    "        y_pred=model.predict(test_generator)\n",
    "        calculo_metricas=metricas(y_test, y_pred) #se llama a la función creada previamente para calcular las métricas de cada modelo\n",
    "        #se calcula loss a partir de la evaluación del modelo\n",
    "        loss=model.evaluate(test_generator, verbose=0)[0]\n",
    "        \n",
    "        #esto es en caso de querer meter todos estos parametros dentro de metricas (cambiando tambien la linea de arriba, en lugar de metricas loss, accuracy...)\n",
    "        #metricas = f\"Loss: {loss}, Accuracy: {accuracy}, Recall: {recall}, AUC: {AUC}, Precision: {precision}\"\n",
    "\n",
    "        #cambiar .append por .concat\n",
    "        #se añaden todos los componentes necesarios para comparar los distintos modelos de arquitectura para distintos batch size \n",
    "        #(comparando las métricas)\n",
    "        compara_arqu_batch=compara_arqu_batch.append({\"Red\": modelo, \"BatchSize\": batch_size, \"Loss\": loss, \"Accuracy\": calculo_metricas[0], \"Precision\": calculo_metricas[1], \"Recall\": calculo_metricas[2], \"F1\":calculo_metricas[3], \"Specificity\":calculo_metricas[4], \"fpr\":calculo_metricas[5], \"fnr\":calculo_metricas[6], \"AUC\": calculo_metricas[7]}, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf165ac-313e-4ee2-ae91-2ba544fea8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#se fijan las columnas A y B como índices. \n",
    "compara_arqu_batch.set_index([\"Red\",\"BatchSize\"], inplace=True) #inplace=True se pone para modificar el dataframe original ya que sino, no se modifica\n",
    "compara_arqu_batch_def = compara_arqu_batch.round(2) #se redondean los decimales a 2\n",
    "compara_arqu_batch_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa112d-b0a8-49ec-88e0-7a277b14ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por lo tanto, se puede deducir que, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52955cda-cfa5-49dc-ae72-33c901dc30ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea38f30-5183-48e3-9678-713040bb57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "batch_sizes=[8, 16, 20, 32, 64]  # distintos tamaños de batch size para probar\n",
    "modelos=[\"Simple1\", \"Simple2\", \"Simple3\"]  # Lista de nombres de modelos\n",
    "\n",
    "#se inicializa un dataframe vacío donde, posteriormente se van a añadir todos los componentes necesarios para comparar los distintos \n",
    "#modelos de arquitectura para distintos batch size (comparando las métricas)\n",
    "compara_arqu_batch=pd.DataFrame()\n",
    "\n",
    "epochs=5\n",
    "\n",
    "#bucle en el que se recorren cada uno de los modelos y los tamaños de batch_size \n",
    "for modelo in modelos:\n",
    "    print(f\"Comparando modelo {modelo}...\")\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Entrenando modelo {modelo} y batch_size {batch_size}\")\n",
    "        \n",
    "        #se emplea la función establecer_arquitectura para determinar el modelo con el que se trabaja cada vez\n",
    "        model = establecer_arquitectura(modelo)\n",
    "        \n",
    "        #se compila el modelo y se calculan las métricas con las que se quiere trabajar\n",
    "        #en este caso, en la función de pérdida \"loss\", se emplea la entropía cruzada binaria \"binary_crossentropy\" ya que se trata de \n",
    "        #un problema de clasificación binaria\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",\"Recall\"]) #cambias loss\n",
    "\n",
    "        #ENTRENA\n",
    "        # con callbacks se detiene el entrenamiento si la pérdida en el conjunto de validación no mejora después de 3 épocas (patience)\n",
    "        model.fit(train_generator, batch_size=batch_size, epochs=epochs, validation_data=validation_generator, callbacks=EarlyStopping(monitor='val_loss', patience=3))\n",
    "\n",
    "        #se calculan las métricas\n",
    "        y_test=test_generator.labels\n",
    "        y_pred=model.predict(test_generator)\n",
    "        calculo_metricas=metricas(y_test, y_pred) #se llama a la función creada previamente para calcular las métricas de cada modelo\n",
    "        #se calcula loss a partir de la evaluación del modelo\n",
    "        loss=model.evaluate(test_generator, verbose=0)[0]\n",
    "\n",
    "        #esto es en caso de querer meter todos estos parametros dentro de metricas (cambiando tambien la linea de arriba, en lugar de metricas loss, accuracy...)\n",
    "        #metricas = f\"Loss: {loss}, Accuracy: {accuracy}, Recall: {recall}, AUC: {AUC}, Precision: {precision}\"\n",
    "\n",
    "        #cambiar .append por .concat\n",
    "        #se añaden todos los componentes necesarios para comparar los distintos modelos de arquitectura para distintos batch size \n",
    "        #(comparando las métricas)\n",
    "        compara_arqu_batch=compara_arqu_batch.append({\"Red\": modelo, \"BatchSize\": batch_size, \"Loss\": loss, \"Accuracy\": calculo_metricas[0], \"Precision\": calculo_metricas[1], \"Recall\": calculo_metricas[2], \"F1\":calculo_metricas[3], \"Specificity\":calculo_metricas[4], \"fpr\":calculo_metricas[5], \"fnr\":calculo_metricas[6], \"AUC\": calculo_metricas[7]}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184a38d-61b1-4ed9-a293-db79ba086e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''compara_arqu_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90df82-9a99-422b-8cc2-7237d372c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por lo tanto, se puede apreciar que la primera arquitectura es la peor de todas (algo que era de esperar) y la mejor opcion es \n",
    "# el Simple 2 ya que, en general (exceptuando para una batch size de 8) obtiene mejores resultados.\n",
    "#Dentro del Simple 2, el mejor valor de batch size es el de 32 ya que es el que presenta mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ed9a0-e739-4669-9391-305a3d45dacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c7252-e239-45b8-bc01-6df22183405c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0b8a3-f70f-41ff-822a-b956996a6623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d15853-36f9-4980-b52a-7b824f0a700b",
   "metadata": {},
   "source": [
    "## Comparación de distintos valores de número de neuronas para la arquitectura \"Simple2\" y un batchsize de 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c884c-043f-451a-b8a2-34f801293780",
   "metadata": {},
   "source": [
    "A partir de los resultados obtenidos previamente, se comparan distintos valores de número de neuronas para determinar con cuál funciona mejor el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563679f-a18b-4449-a256-59ec9f249590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#antes hay que ejecutar train generatos y la funcion de metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bc8de-be58-4b81-93ad-b5426cd4b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def neuronas(num_neuronas, epochs, ruta, batch_size):\n",
    "\n",
    "    '''\n",
    "    Función que devuelve una tabla comparativa para distintas valores de neuronas introducidos como parámetros a partir del modelo y el batch size\n",
    "    seleccionado previamente.\n",
    "    ------------------------------------------------------------------------\n",
    "    Parámetros;\n",
    "    - num_neuronas:\n",
    "    - epochs:\n",
    "    - ruta: str. Ruta base donde se encuentran las imágenes organizadas en subcarpetas (train, val, test)\n",
    "    - batch_size: int. Tamaño del lote que se utiliza en una única iteración del algoritmo de aprendizaje. Se emplea dentro de la función\n",
    "    \"preparar_modelo\" para determinar el tamaño del lote para cada uno de los generadores (train, val y test)\n",
    "    ----------------------------------------------------------------\n",
    "    Return:\n",
    "    - compara_neuronas_def: dataframe que contiene como índice las columnas referidas al número de neuronas. El dataframe \n",
    "    obtenido se observa como una tabla comparativa de diversas métricas para cada número de neuronas.\n",
    "    '''\n",
    "    \n",
    "    #se inicializa un dataframe vacío donde, posteriormente se van a añadir todos los componentes necesarios para comparar y determinar cual es el mejor\n",
    "    #valor de neuronas en la capa oculta\n",
    "    compara_neuronas=pd.DataFrame()\n",
    "    \n",
    "    input_shape=(150,150,3)\n",
    "\n",
    "    #se emplea la función preparar_modelo para configurar los generadores de datos para entrenar, validar y probar \n",
    "    #un modelo de aprendizaje automático con imágenes\n",
    "    preparar_modelo(ruta, batch_size)\n",
    "    \n",
    "    for neurona in num_neuronas:\n",
    "        print(f\"Modelo con {neurona} neuronas en su capa oculta...\")\n",
    "    \n",
    "        #se emplea el modelo Simple2 que es el que se ha determinado previamente como \"mejor\"\n",
    "        model = keras.Sequential(\n",
    "                [\n",
    "                    keras.Input(shape=input_shape),\n",
    "                    layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                    layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "                    layers.Flatten(), #convierte imágenes en vectores\n",
    "                    layers.Dense(neurona, activation=\"relu\"), #se va cambiando el valor de \"neurona\" para cada uno de los valores que estan en la lista num_neuronas\n",
    "                    layers.Dropout(0.2),\n",
    "                    layers.Dense(1, activation=\"sigmoid\"), #produce una probabilidad entre 0 y 1 para la clasificación binaria\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        #se compila el modelo y se calculan las métricas con las que se quiere trabajar\n",
    "        #en este caso, en la función de pérdida \"loss\", se emplea la entropía cruzada binaria \"binary_crossentropy\" ya que se trata de \n",
    "        #un problema de clasificación binaria\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",\"Recall\"]) #cambias loss\n",
    "    \n",
    "        #ENTRENA\n",
    "        # con callbacks se detiene el entrenamiento si la pérdida en el conjunto de validación no mejora después de 10 épocas (patience)\n",
    "        #se emplea un batch size de 32 que es el que ha dado mejores resultados antes\n",
    "        model.fit(train_generator, batch_size=32, epochs=epochs, validation_data=validation_generator, callbacks=EarlyStopping(monitor='val_auc', patience=10,restore_best_weights=True))\n",
    "    \n",
    "        #se calculan las métricas\n",
    "        y_test=test_generator.labels\n",
    "        y_pred=model.predict(test_generator)\n",
    "        calculo_metricas=metricas(y_test, y_pred) #se llama a la función creada previamente para calcular las métricas de cada modelo\n",
    "        #se calcula loss a partir de la evaluación del modelo\n",
    "        loss=model.evaluate(test_generator, verbose=0)[0]\n",
    "    \n",
    "        #esto es en caso de querer meter todos estos parametros dentro de metricas (cambiando tambien la linea de arriba, en lugar de metricas loss, accuracy...)\n",
    "        #metricas = f\"Loss: {loss}, Accuracy: {accuracy}, Recall: {recall}, AUC: {AUC}, Precision: {precision}\"\n",
    "    \n",
    "        #cambiar .append por .concat\n",
    "        #se añaden todos los componentes necesarios para comparar los distintos modelos de arquitectura para distintos batch size \n",
    "        #(comparando las métricas)\n",
    "        compara_neuronas=compara_neuronas.append({\"Número de neuronas\": neurona, \"Loss\": loss, \"Accuracy\": calculo_metricas[0], \"Precision\": calculo_metricas[1], \"Recall\": calculo_metricas[2], \"F1\":calculo_metricas[3], \"Specificity\":calculo_metricas[4], \"fpr\":calculo_metricas[5], \"fnr\":calculo_metricas[6], \"AUC\": calculo_metricas[7]}, ignore_index=True)\n",
    "    \n",
    "    #se fija la columna \"Número de neuronas\" como índice. \n",
    "    compara_neuronas.set_index(\"Número de neuronas\", inplace=True) #inplace=True se pone para modificar el dataframe original ya que sino, no se modifica\n",
    "    compara_neuronas_def = compara_neuronas.round(2) #se redondean los decimales a 2\n",
    "    return compara_neuronas_def\n",
    "    \n",
    "        #PONER int(neurona) si salen como decimanles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36fa74-69d2-4500-9d22-868cf9b384bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neuronas=[32, 64, 128, 156] #lista con distintos valores de neuronas para probar\n",
    "epochs=20\n",
    "ruta='C:/Users/nuria/Downloads/TFG/data_nuevo'\n",
    "batchsize=20\n",
    "\n",
    "neuronas(num_neuronas, epochs, ruta, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa1344-69d6-4800-8201-9fa62b2be003",
   "metadata": {},
   "source": [
    "Por lo tanto, se puede apreciar que, el mejor modelo se corresponde con 64 neuronas en la capa oculta ya que, \n",
    "tiene un valor mayor en la gran parte de métricas (aunque en loss deberia ser menor) CAMBIAR EN CASO NECESARIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70ad44-41bf-4c21-897f-5dfa8f83fe0d",
   "metadata": {},
   "source": [
    "## Matriz de confusión para ver como funciona el modelo elegido finalmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf0aa3-f29f-4b6a-bd79-4aec32a7e8dd",
   "metadata": {},
   "source": [
    "Finalmente, se obtiene la matriz de confusión para el modelo final obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215b28e-359b-45d2-9bd3-07a0919a8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAMBIAR TODO LO QUE HAY A CONTINUACION POR EL MODELO FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4a642-c3d4-4064-a4b0-0e2b18044a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta='C:/Users/nuria/Downloads/TFG/data'\n",
    "batchsize=20\n",
    "preparar_modelo(ruta, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab559147-c68c-4bdf-8678-466cbdc88038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#se trabaja con el modelo simple1\n",
    "input_shape=(150,150,3)\n",
    "\n",
    "model = keras.Sequential( #funcion establecer arquitectura(simple...)\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"), \n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        #se necesitan mas capas\n",
    "        layers.Dropout(0.5), #probar otros valores (este es muy alto)\n",
    "        layers.Dense(1, activation=\"sigmoid\"), #una unica neurina, sigmoide\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746296f1-e216-486d-aabd-80d626ac8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "batch_size = 32 #jugar un poco con este parametro (potencias de 2) (cuanto mayor sea el valor menos tarda)\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\",\"Recall\"]) #cambias loss\n",
    "\n",
    "# con callbacks se detiene el entrenamiento si la pérdida en el conjunto de validación no mejora después de 3 épocas (patience)\n",
    "model.fit(train_generator, epochs=epochs, validation_data=validation_generator, callbacks=EarlyStopping(monitor='val_auc', patience=5, save_best_only=True, start_from_epoch=5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009464a-c8a2-4e3a-a033-0b0aa6465ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=test_generator.labels\n",
    "y_pred=model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ad034-109d-42ea-84b4-950095a5bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=y_pred>0.5 #para convertirlo en un problema binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00f04a-bdf6-458f-b926-3a6ff26d7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matriz de confusión con sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matriz = confusion_matrix(y_test, y_pred) #.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd24bd6-2635-472a-b10e-facbf5705f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d5d03-8012-46fe-bac0-d9e7cff004d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERCEPTRON SKLEARN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import numpy as np \n",
    "\n",
    "labels=np.unique(y_test)\n",
    "\n",
    "matriz_conf = metrics.confusion_matrix(y_test, y_pred,labels=labels)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = matriz_conf, display_labels = [\"neumonía\" , \"no neumonía\"])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "cm_display.plot(ax=ax)\n",
    "plt.title(\"Matriz de confusión neumonía-no neumonía\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82d04a-c14b-4970-bc4c-f5c2924c6baf",
   "metadata": {},
   "source": [
    "Se puede comprobar como han mejorado los resultados respecto al modelo más simple ya que..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1115865-dddc-4ec1-bb3a-efa3419117a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b2ecb-81e3-40a9-9fcb-eec582cd18fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARTICULO METIDO EN EL RESUMEN DE LA MEMORIA\n",
    "#https://www.sciencedirect.com/science/article/pii/S001048252030247X?via%3Dihub#bib1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
